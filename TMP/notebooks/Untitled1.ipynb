{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'Opening Files.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Principal Component Analysis Base Classes\"\"\"\n",
    "\n",
    "# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#         Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "#         Denis A. Engemann <denis-alexander.engemann@inria.fr>\n",
    "#         Kyle Kastner <kastnerkyle@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.externals import six\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "\n",
    "class _BasePCA(six.with_metaclass(ABCMeta, BaseEstimator, TransformerMixin)):\n",
    "    \"\"\"Base class for PCA methods.\n",
    "    Warning: This class should not be used directly.\n",
    "    Use derived classes instead.\n",
    "    \"\"\"\n",
    "    def get_covariance(self):\n",
    "        \"\"\"Compute data covariance with the generative model.\n",
    "        ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n",
    "        where  S**2 contains the explained variances, and sigma2 contains the\n",
    "        noise variances.\n",
    "        Returns\n",
    "        -------\n",
    "        cov : array, shape=(n_features, n_features)\n",
    "            Estimated covariance of data.\n",
    "        \"\"\"\n",
    "        components_ = self.components_\n",
    "        exp_var = self.explained_variance_\n",
    "        if self.whiten:\n",
    "            components_ = components_ * np.sqrt(exp_var[:, np.newaxis])\n",
    "        exp_var_diff = np.maximum(exp_var - self.noise_variance_, 0.)\n",
    "        cov = np.dot(components_.T * exp_var_diff, components_)\n",
    "        cov.flat[::len(cov) + 1] += self.noise_variance_  # modify diag inplace\n",
    "        return cov\n",
    "\n",
    "    def get_precision(self):\n",
    "        \"\"\"Compute data precision matrix with the generative model.\n",
    "        Equals the inverse of the covariance but computed with\n",
    "        the matrix inversion lemma for efficiency.\n",
    "        Returns\n",
    "        -------\n",
    "        precision : array, shape=(n_features, n_features)\n",
    "            Estimated precision of data.\n",
    "        \"\"\"\n",
    "        n_features = self.components_.shape[1]\n",
    "\n",
    "        # handle corner cases first\n",
    "        if self.n_components_ == 0:\n",
    "            return np.eye(n_features) / self.noise_variance_\n",
    "        if self.n_components_ == n_features:\n",
    "            return linalg.inv(self.get_covariance())\n",
    "\n",
    "        # Get precision using matrix inversion lemma\n",
    "        components_ = self.components_\n",
    "        exp_var = self.explained_variance_\n",
    "        if self.whiten:\n",
    "            components_ = components_ * np.sqrt(exp_var[:, np.newaxis])\n",
    "        exp_var_diff = np.maximum(exp_var - self.noise_variance_, 0.)\n",
    "        precision = np.dot(components_, components_.T) / self.noise_variance_\n",
    "        precision.flat[::len(precision) + 1] += 1. / exp_var_diff\n",
    "        precision = np.dot(components_.T,\n",
    "                           np.dot(linalg.inv(precision), components_))\n",
    "        precision /= -(self.noise_variance_ ** 2)\n",
    "        precision.flat[::len(precision) + 1] += 1. / self.noise_variance_\n",
    "        return precision\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(X, y=None):\n",
    "        \"\"\"Placeholder for fit. Subclasses should implement this method!\n",
    "        Fit the model with X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply dimensionality reduction to X.\n",
    "        X is projected on the first principal components previously extracted\n",
    "        from a training set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            New data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import numpy as np\n",
    "        >>> from sklearn.decomposition import IncrementalPCA\n",
    "        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
    "        >>> ipca.fit(X)\n",
    "        IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False)\n",
    "        >>> ipca.transform(X) # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['mean_', 'components_'], all_or_any=all)\n",
    "\n",
    "        X = check_array(X)\n",
    "        if self.mean_ is not None:\n",
    "            X = X - self.mean_\n",
    "        X_transformed = np.dot(X, self.components_.T)\n",
    "        if self.whiten:\n",
    "            X_transformed /= np.sqrt(self.explained_variance_)\n",
    "        return X_transformed\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Transform data back to its original space.\n",
    "        In other words, return an input X_original whose transform would be X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_components)\n",
    "            New data, where n_samples is the number of samples\n",
    "            and n_components is the number of components.\n",
    "        Returns\n",
    "        -------\n",
    "        X_original array-like, shape (n_samples, n_features)\n",
    "        Notes\n",
    "        -----\n",
    "        If whitening is enabled, inverse_transform will compute the\n",
    "        exact inverse operation, which includes reversing whitening.\n",
    "        \"\"\"\n",
    "        if self.whiten:\n",
    "            return np.dot(X, np.sqrt(self.explained_variance_[:, np.newaxis]) *\n",
    "                            self.components_) + self.mean_\n",
    "        else:\n",
    "            return np.dot(X, self.components_) + self.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import log, sqrt\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from scipy.special import gammaln\n",
    "from scipy.sparse import issparse\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from sklearn.externals import six\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import deprecated\n",
    "from sklearn.utils import check_random_state, as_float_array\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.extmath import fast_logdet, randomized_svd, svd_flip\n",
    "from sklearn.utils.extmath import stable_cumsum\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PCA(_BasePCA):\n",
    "\n",
    "\n",
    "    def __init__(self, n_components=None, copy=True, whiten=False,\n",
    "                 svd_solver='auto', tol=0.0, iterated_power='auto',\n",
    "                 random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.copy = copy\n",
    "        self.whiten = whiten\n",
    "        self.svd_solver = svd_solver\n",
    "        self.tol = tol\n",
    "        self.iterated_power = iterated_power\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model with X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples in the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : Ignored.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        self._fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit the model with X and apply the dimensionality reduction on X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : Ignored.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        U, S, V = self._fit(X)\n",
    "        U = U[:, :self.n_components_]\n",
    "\n",
    "        if self.whiten:\n",
    "            # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\n",
    "            U *= sqrt(X.shape[0] - 1)\n",
    "        else:\n",
    "            # X_new = X * V = U * S * V^T * V = U * S\n",
    "            U *= S[:self.n_components_]\n",
    "\n",
    "        return U\n",
    "\n",
    "    def _fit(self, X):\n",
    "        \"\"\"Dispatch to the right submethod depending on the chosen solver.\"\"\"\n",
    "\n",
    "        # Raise an error for sparse input.\n",
    "        # This is more informative than the generic one raised by check_array.\n",
    "        if issparse(X):\n",
    "            raise TypeError('PCA does not support sparse input. See '\n",
    "                            'TruncatedSVD for a possible alternative.')\n",
    "\n",
    "        X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n",
    "                        copy=self.copy)\n",
    "\n",
    "        # Handle n_components==None\n",
    "        if self.n_components is None:\n",
    "            n_components = X.shape[1]\n",
    "        else:\n",
    "            n_components = self.n_components\n",
    "\n",
    "        # Handle svd_solver\n",
    "        svd_solver = self.svd_solver\n",
    "        if svd_solver == 'auto':\n",
    "            # Small problem, just call full PCA\n",
    "            if max(X.shape) <= 500:\n",
    "                svd_solver = 'full'\n",
    "            elif n_components >= 1 and n_components < .8 * min(X.shape):\n",
    "                svd_solver = 'randomized'\n",
    "            # This is also the case of n_components in (0,1)\n",
    "            else:\n",
    "                svd_solver = 'full'\n",
    "\n",
    "        # Call different fits for either full or truncated SVD\n",
    "        if svd_solver == 'full':\n",
    "            return self._fit_full(X, n_components)\n",
    "        elif svd_solver in ['arpack', 'randomized']:\n",
    "            return self._fit_truncated(X, n_components, svd_solver)\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized svd_solver='{0}'\"\n",
    "                             \"\".format(svd_solver))\n",
    "\n",
    "    def _fit_full(self, X, n_components):\n",
    "        \"\"\"Fit the model by computing full SVD on X\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if n_components == 'mle':\n",
    "            if n_samples < n_features:\n",
    "                raise ValueError(\"n_components='mle' is only supported \"\n",
    "                                 \"if n_samples >= n_features\")\n",
    "        elif not 0 <= n_components <= n_features:\n",
    "            raise ValueError(\"n_components=%r must be between 0 and \"\n",
    "                             \"n_features=%r with svd_solver='full'\"\n",
    "                             % (n_components, n_features))\n",
    "\n",
    "        # Center data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X -= self.mean_\n",
    "\n",
    "        U, S, V = linalg.svd(X, full_matrices=False)\n",
    "        # flip eigenvectors' sign to enforce deterministic output\n",
    "        U, V = svd_flip(U, V)\n",
    "\n",
    "        components_ = V\n",
    "        print S\n",
    "        # Get variance explained by singular values\n",
    "        explained_variance_ = (S ** 2) / (n_samples - 1)\n",
    "\n",
    "        total_var = explained_variance_.sum()\n",
    "        explained_variance_ratio_ = explained_variance_ / total_var\n",
    "        singular_values_ = S.copy()  # Store the singular values.\n",
    "\n",
    "        # Postprocess the number of components required\n",
    "        if n_components == 'mle':\n",
    "            n_components = \\\n",
    "                _infer_dimension_(explained_variance_, n_samples, n_features)\n",
    "        elif 0 < n_components < 1.0:\n",
    "            # number of components for which the cumulated explained\n",
    "            # variance percentage is superior to the desired threshold\n",
    "            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\n",
    "            n_components = np.searchsorted(ratio_cumsum, n_components) + 1\n",
    "\n",
    "        # Compute noise covariance using Probabilistic PCA model\n",
    "        # The sigma2 maximum likelihood (cf. eq. 12.46)\n",
    "        if n_components < min(n_features, n_samples):\n",
    "            self.noise_variance_ = explained_variance_[n_components:].mean()\n",
    "        else:\n",
    "            self.noise_variance_ = 0.\n",
    "\n",
    "        self.n_samples_, self.n_features_ = n_samples, n_features\n",
    "        self.components_ = components_[:n_components]\n",
    "        self.n_components_ = n_components\n",
    "        self.explained_variance_ = explained_variance_[:n_components]\n",
    "        self.explained_variance_ratio_ = \\\n",
    "            explained_variance_ratio_[:n_components]\n",
    "        self.singular_values_ = singular_values_[:n_components]\n",
    "\n",
    "        return U, S, V\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.61239429e+03 3.74813514e+03 2.35215768e+03 1.77558230e+03\n",
      " 1.10326788e+03 9.12427481e+02 5.67603375e+02 5.41925311e+02\n",
      " 3.06298170e+02 2.79849575e+02 2.60004884e+02 1.29189279e+02\n",
      " 8.55535356e+01 7.78399632e+01 7.21989731e+01 7.01963659e+01\n",
      " 5.99213580e+01 5.63053016e+01 5.58115684e+01 5.38358571e+01\n",
      " 4.88140205e+01 4.60424721e+01 4.18124003e+01 3.79807973e+01\n",
      " 3.63473085e+01 3.47964451e+01 3.19094006e+01 3.13348661e+01\n",
      " 3.00299345e+01 2.61791338e+01 2.51800550e+01 2.49474547e+01\n",
      " 2.37788865e+01 2.21570309e+01 2.14153737e+01 1.81606804e+01\n",
      " 1.70251988e+01 1.46037219e+01 1.42509015e+01 1.34360173e+01\n",
      " 1.19066697e+01 1.11666765e+01 9.16494078e+00 8.00025403e+00\n",
      " 5.90983419e+00 2.49240375e-12 4.48798132e-13 4.48798132e-13\n",
      " 4.48798132e-13 4.48798132e-13 4.48798132e-13 4.48798132e-13\n",
      " 4.48798132e-13 4.48798132e-13 4.48798132e-13 4.48798132e-13\n",
      " 4.48798132e-13 4.48798132e-13 4.48798132e-13 4.48798132e-13\n",
      " 4.48798132e-13 4.48798132e-13 4.48798132e-13 4.48798132e-13]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=64)\n",
    "    \n",
    "\n",
    "S_=pca.fit_transform(matriz_serpentina)  \n",
    "A_=pca.components_\n",
    "scores = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.17234274e+05, 9.68863245e+04, 3.81561775e+04, 2.17427070e+04,\n",
       "       8.39448282e+03, 5.74154420e+03, 2.22188684e+03, 2.02540030e+03,\n",
       "       6.47024612e+02, 5.40108859e+02, 4.66224412e+02, 1.15102550e+02,\n",
       "       5.04786721e+01, 4.17866199e+01, 3.59495980e+01, 3.39829640e+01,\n",
       "       2.47625458e+01, 2.18640482e+01, 2.14822839e+01, 1.99882725e+01,\n",
       "       1.64331627e+01, 1.46200637e+01, 1.20570815e+01, 9.94855836e+00,\n",
       "       9.11121957e+00, 8.35029372e+00, 7.02213688e+00, 6.77154369e+00,\n",
       "       6.21928940e+00, 4.72653134e+00, 4.37265633e+00, 4.29224479e+00,\n",
       "       3.89955477e+00, 3.38575186e+00, 3.16288435e+00, 2.27455389e+00,\n",
       "       1.99901651e+00, 1.47081857e+00, 1.40060823e+00, 1.24501077e+00,\n",
       "       9.77715745e-01, 8.59963195e-01, 5.79283721e-01, 4.41407341e-01,\n",
       "       2.40869932e-01, 4.28419065e-26, 1.38910182e-27, 1.38910182e-27,\n",
       "       1.38910182e-27, 1.38910182e-27, 1.38910182e-27, 1.38910182e-27,\n",
       "       1.38910182e-27, 1.38910182e-27, 1.38910182e-27, 1.38910182e-27,\n",
       "       1.38910182e-27, 1.38910182e-27, 1.38910182e-27, 1.38910182e-27,\n",
       "       1.38910182e-27, 1.38910182e-27, 1.38910182e-27, 1.38910182e-27])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
