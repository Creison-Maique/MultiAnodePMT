{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the fast ICA algorithms.\n",
    "Reference: Tables 8.3 and 8.4 page 196 in the book:\n",
    "Independent Component Analysis, by  Hyvarinen et al.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Pierre Lafaye de Micheaux, Stefan van der Walt, Gael Varoquaux,\n",
    "#          Bertrand Thirion, Alexandre Gramfort, Denis A. Engemann\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import six\n",
    "from sklearn.externals.six import moves\n",
    "from sklearn.externals.six import string_types\n",
    "from sklearn.utils import check_array, as_float_array, check_random_state\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.validation import FLOAT_DTYPES\n",
    "\n",
    "__all__ = ['fastica', 'FastICA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _gs_decorrelation(w, W, j):\n",
    "    \"\"\"\n",
    "    Orthonormalize w wrt the first j rows of W\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray of shape(n)\n",
    "        Array to be orthogonalized\n",
    "    W : ndarray of shape(p, n)\n",
    "        Null space definition\n",
    "    j : int < p\n",
    "        The no of (from the first) rows of Null space W wrt which w is\n",
    "        orthogonalized.\n",
    "    Notes\n",
    "    -----\n",
    "    Assumes that W is orthogonal\n",
    "    w changed in place\n",
    "    \"\"\"\n",
    "    w -= np.dot(np.dot(w, W[:j].T), W[:j])\n",
    "    return w\n",
    "\n",
    "\n",
    "def _sym_decorrelation(W):\n",
    "    \"\"\" Symmetric decorrelation\n",
    "    i.e. W <- (W * W.T) ^{-1/2} * W\n",
    "    \"\"\"\n",
    "    s, u = linalg.eigh(np.dot(W, W.T))\n",
    "    # u (resp. s) contains the eigenvectors (resp. square roots of\n",
    "    # the eigenvalues) of W * W.T\n",
    "   \n",
    "    return np.dot(np.dot(u * (1. / np.sqrt(s)), u.T), W)\n",
    "\n",
    "\n",
    "def _ica_def(X, tol, g, fun_args, max_iter, w_init):\n",
    "    \"\"\"Deflationary FastICA using fun approx to neg-entropy function\n",
    "    Used internally by FastICA.\n",
    "    \"\"\"\n",
    "\n",
    "    n_components = w_init.shape[0]\n",
    "    W = np.zeros((n_components, n_components), dtype=X.dtype)\n",
    "    n_iter = []\n",
    "\n",
    "    # j is the index of the extracted component\n",
    "    for j in range(n_components):\n",
    "        w = w_init[j, :].copy()\n",
    "        w /= np.sqrt((w ** 2).sum())\n",
    "\n",
    "        for i in moves.xrange(max_iter):\n",
    "            gwtx, g_wtx = g(np.dot(w.T, X), fun_args)\n",
    "\n",
    "            w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\n",
    "\n",
    "            _gs_decorrelation(w1, W, j)\n",
    "\n",
    "            w1 /= np.sqrt((w1 ** 2).sum())\n",
    "\n",
    "            lim = np.abs(np.abs((w1 * w).sum()) - 1)\n",
    "            w = w1\n",
    "            if lim < tol:\n",
    "                break\n",
    "\n",
    "        n_iter.append(i + 1)\n",
    "        W[j, :] = w\n",
    "\n",
    "    return W, max(n_iter)\n",
    "\n",
    "\n",
    "def _ica_par(X, tol, g, fun_args, max_iter, w_init):\n",
    "    \"\"\"Parallel FastICA.\n",
    "    Used internally by FastICA --main loop\n",
    "    \"\"\"\n",
    "    W = _sym_decorrelation(w_init)\n",
    "    del w_init\n",
    "    p_ = float(X.shape[1])\n",
    "    for ii in moves.xrange(max_iter):\n",
    "        gwtx, g_wtx = g(np.dot(W, X), fun_args)\n",
    "        W1 = _sym_decorrelation(np.dot(gwtx, X.T) / p_\n",
    "                                - g_wtx[:, np.newaxis] * W)\n",
    "        del gwtx, g_wtx\n",
    "        # builtin max, abs are faster than numpy counter parts.\n",
    "        lim = max(abs(abs(np.diag(np.dot(W1, W.T))) - 1))\n",
    "        W = W1\n",
    "        if lim < tol:\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn('FastICA did not converge. Consider increasing '\n",
    "                      'tolerance or the maximum number of iterations.')\n",
    "   \n",
    "    return W, ii + 1\n",
    "\n",
    "\n",
    "# Some standard non-linear functions.\n",
    "# XXX: these should be optimized, as they can be a bottleneck.\n",
    "def _logcosh(x, fun_args=None):\n",
    "    alpha = fun_args.get('alpha', 1.0)  # comment it out?\n",
    "\n",
    "    x *= alpha\n",
    "    gx = np.tanh(x, x)  # apply the tanh inplace\n",
    "    g_x = np.empty(x.shape[0])\n",
    "    # XXX compute in chunks to avoid extra allocation\n",
    "    for i, gx_i in enumerate(gx):  # please don't vectorize.\n",
    "        g_x[i] = (alpha * (1 - gx_i ** 2)).mean()\n",
    "    return gx, g_x\n",
    "\n",
    "\n",
    "def _exp(x, fun_args):\n",
    "    exp = np.exp(-(x ** 2) / 2)\n",
    "    gx = x * exp\n",
    "    g_x = (1 - x ** 2) * exp\n",
    "    return gx, g_x.mean(axis=-1)\n",
    "\n",
    "\n",
    "def _cube(x, fun_args):\n",
    "    return x ** 3, (3 * x ** 2).mean(axis=-1)\n",
    "\n",
    "\n",
    "def fastica(X, n_components=None, algorithm=\"parallel\", whiten=True,\n",
    "            fun=\"logcosh\", fun_args=None, max_iter=200, tol=1e-04, w_init=None,\n",
    "            random_state=None, return_X_mean=False, compute_sources=True,\n",
    "            return_n_iter=False):\n",
    "    \"\"\"Perform Fast Independent Component Analysis.\n",
    "    Read more in the :ref:`User Guide <ICA>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    n_components : int, optional\n",
    "        Number of components to extract. If None no dimension reduction\n",
    "        is performed.\n",
    "    algorithm : {'parallel', 'deflation'}, optional\n",
    "        Apply a parallel or deflational FASTICA algorithm.\n",
    "    whiten : boolean, optional\n",
    "        If True perform an initial whitening of the data.\n",
    "        If False, the data is assumed to have already been\n",
    "        preprocessed: it should be centered, normed and white.\n",
    "        Otherwise you will get incorrect results.\n",
    "        In this case the parameter n_components will be ignored.\n",
    "    fun : string or function, optional. Default: 'logcosh'\n",
    "        The functional form of the G function used in the\n",
    "        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n",
    "        or 'cube'.\n",
    "        You can also provide your own function. It should return a tuple\n",
    "        containing the value of the function, and of its derivative, in the\n",
    "        point. Example:\n",
    "        def my_g(x):\n",
    "            return x ** 3, 3 * x ** 2\n",
    "    fun_args : dictionary, optional\n",
    "        Arguments to send to the functional form.\n",
    "        If empty or None and if fun='logcosh', fun_args will take value\n",
    "        {'alpha' : 1.0}\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations to perform.\n",
    "    tol : float, optional\n",
    "        A positive scalar giving the tolerance at which the\n",
    "        un-mixing matrix is considered to have converged.\n",
    "    w_init : (n_components, n_components) array, optional\n",
    "        Initial un-mixing array of dimension (n.comp,n.comp).\n",
    "        If None (default) then an array of normal r.v.'s is used.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    return_X_mean : bool, optional\n",
    "        If True, X_mean is returned too.\n",
    "    compute_sources : bool, optional\n",
    "        If False, sources are not computed, but only the rotation matrix.\n",
    "        This can save memory when working with big data. Defaults to True.\n",
    "    return_n_iter : bool, optional\n",
    "        Whether or not to return the number of iterations.\n",
    "    Returns\n",
    "    -------\n",
    "    K : array, shape (n_components, n_features) | None.\n",
    "        If whiten is 'True', K is the pre-whitening matrix that projects data\n",
    "        onto the first n_components principal components. If whiten is 'False',\n",
    "        K is 'None'.\n",
    "    W : array, shape (n_components, n_components)\n",
    "        Estimated un-mixing matrix.\n",
    "        The mixing matrix can be obtained by::\n",
    "            w = np.dot(W, K.T)\n",
    "            A = w.T * (w * w.T).I\n",
    "    S : array, shape (n_samples, n_components) | None\n",
    "        Estimated source matrix\n",
    "    X_mean : array, shape (n_features, )\n",
    "        The mean over features. Returned only if return_X_mean is True.\n",
    "    n_iter : int\n",
    "        If the algorithm is \"deflation\", n_iter is the\n",
    "        maximum number of iterations run across all components. Else\n",
    "        they are just the number of iterations taken to converge. This is\n",
    "        returned only when return_n_iter is set to `True`.\n",
    "    Notes\n",
    "    -----\n",
    "    The data matrix X is considered to be a linear combination of\n",
    "    non-Gaussian (independent) components i.e. X = AS where columns of S\n",
    "    contain the independent components and A is a linear mixing\n",
    "    matrix. In short ICA attempts to `un-mix' the data by estimating an\n",
    "    un-mixing matrix W where ``S = W K X.``\n",
    "    This implementation was originally made for data of shape\n",
    "    [n_features, n_samples]. Now the input is transposed\n",
    "    before the algorithm is applied. This makes it slightly\n",
    "    faster for Fortran-ordered input.\n",
    "    Implemented using FastICA:\n",
    "    `A. Hyvarinen and E. Oja, Independent Component Analysis:\n",
    "    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n",
    "    pp. 411-430`\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(random_state)\n",
    "    fun_args = {} if fun_args is None else fun_args\n",
    "    # make interface compatible with other decompositions\n",
    "    # a copy is required only for non whitened data\n",
    "    X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES).T\n",
    "\n",
    "    alpha = fun_args.get('alpha', 1.0)\n",
    "    if not 1 <= alpha <= 2:\n",
    "        raise ValueError('alpha must be in [1,2]')\n",
    "\n",
    "    if fun == 'logcosh':\n",
    "        g = _logcosh\n",
    "    elif fun == 'exp':\n",
    "        g = _exp\n",
    "    elif fun == 'cube':\n",
    "        g = _cube\n",
    "    elif callable(fun):\n",
    "        def g(x, fun_args):\n",
    "            return fun(x, **fun_args)\n",
    "    else:\n",
    "        exc = ValueError if isinstance(fun, six.string_types) else TypeError\n",
    "        raise exc(\"Unknown function %r;\"\n",
    "                  \" should be one of 'logcosh', 'exp', 'cube' or callable\"\n",
    "                  % fun)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    if not whiten and n_components is not None:\n",
    "        n_components = None\n",
    "        warnings.warn('Ignoring n_components with whiten=False.')\n",
    "\n",
    "    if n_components is None:\n",
    "        n_components = min(n, p)\n",
    "    if (n_components > min(n, p)):\n",
    "        n_components = min(n, p)\n",
    "        warnings.warn('n_components is too large: it will be set to %s' % n_components)\n",
    "\n",
    "    if whiten:\n",
    "        # Centering the columns (ie the variables)\n",
    "        X_mean = X.mean(axis=-1)\n",
    "        X -= X_mean[:, np.newaxis]\n",
    "\n",
    "        # Whitening and preprocessing by PCA\n",
    "        u, d, _ = linalg.svd(X, full_matrices=False)\n",
    "\n",
    "        del _\n",
    "        K = (u / d).T[:n_components]  # see (6.33) p.140\n",
    "        del u, d\n",
    "        X1 = np.dot(K, X)\n",
    "        # see (13.6) p.267 Here X1 is white and data\n",
    "        # in X has been projected onto a subspace by PCA\n",
    "        X1 *= np.sqrt(p)\n",
    "    else:\n",
    "        # X must be casted to floats to avoid typing issues with numpy\n",
    "        # 2.0 and the line below\n",
    "        X1 = as_float_array(X, copy=False)  # copy has been taken care of\n",
    "\n",
    "    if w_init is None:\n",
    "        w_init = np.asarray(random_state.normal(size=(n_components,\n",
    "                            n_components)), dtype=X1.dtype)\n",
    "\n",
    "    else:\n",
    "        w_init = np.asarray(w_init)\n",
    "        if w_init.shape != (n_components, n_components):\n",
    "            raise ValueError('w_init has invalid shape -- should be %(shape)s'\n",
    "                             % {'shape': (n_components, n_components)})\n",
    "\n",
    "    kwargs = {'tol': tol,\n",
    "              'g': g,\n",
    "              'fun_args': fun_args,\n",
    "              'max_iter': max_iter,\n",
    "              'w_init': w_init}\n",
    "\n",
    "    if algorithm == 'parallel':\n",
    "        W, n_iter = _ica_par(X1, **kwargs)\n",
    "    elif algorithm == 'deflation':\n",
    "        W, n_iter = _ica_def(X1, **kwargs)\n",
    "    else:\n",
    "        raise ValueError('Invalid algorithm: must be either `parallel` or'\n",
    "                         ' `deflation`.')\n",
    "    del X1\n",
    "\n",
    "    if whiten:\n",
    "        if compute_sources:\n",
    "            S = np.dot(np.dot(W, K), X).T\n",
    "        else:\n",
    "            S = None\n",
    "        if return_X_mean:\n",
    "            if return_n_iter:\n",
    "                return K, W, S, X_mean, n_iter\n",
    "            else:\n",
    "                return K, W, S, X_mean\n",
    "        else:\n",
    "            if return_n_iter:\n",
    "                return K, W, S, n_iter\n",
    "            else:\n",
    "                return K, W, S\n",
    "\n",
    "    else:\n",
    "        if compute_sources:\n",
    "            S = np.dot(W, X).T\n",
    "        else:\n",
    "            S = None\n",
    "        if return_X_mean:\n",
    "            if return_n_iter:\n",
    "                return None, W, S, None, n_iter\n",
    "            else:\n",
    "                return None, W, S, None\n",
    "        else:\n",
    "            if return_n_iter:\n",
    "                return None, W, S, n_iter\n",
    "            else:\n",
    "                return None, W, S\n",
    "\n",
    "\n",
    "class FastICA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"FastICA: a fast algorithm for Independent Component Analysis.\n",
    "    Read more in the :ref:`User Guide <ICA>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of components to use. If none is passed, all are used.\n",
    "    algorithm : {'parallel', 'deflation'}\n",
    "        Apply parallel or deflational algorithm for FastICA.\n",
    "    whiten : boolean, optional\n",
    "        If whiten is false, the data is already considered to be\n",
    "        whitened, and no whitening is performed.\n",
    "    fun : string or function, optional. Default: 'logcosh'\n",
    "        The functional form of the G function used in the\n",
    "        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n",
    "        or 'cube'.\n",
    "        You can also provide your own function. It should return a tuple\n",
    "        containing the value of the function, and of its derivative, in the\n",
    "        point. Example:\n",
    "        def my_g(x):\n",
    "            return x ** 3, 3 * x ** 2\n",
    "    fun_args : dictionary, optional\n",
    "        Arguments to send to the functional form.\n",
    "        If empty and if fun='logcosh', fun_args will take value\n",
    "        {'alpha' : 1.0}.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations during fit.\n",
    "    tol : float, optional\n",
    "        Tolerance on update at each iteration.\n",
    "    w_init : None of an (n_components, n_components) ndarray\n",
    "        The mixing matrix to be used to initialize the algorithm.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    Attributes\n",
    "    ----------\n",
    "    components_ : 2D array, shape (n_components, n_features)\n",
    "        The unmixing matrix.\n",
    "    mixing_ : array, shape (n_features, n_components)\n",
    "        The mixing matrix.\n",
    "    n_iter_ : int\n",
    "        If the algorithm is \"deflation\", n_iter is the\n",
    "        maximum number of iterations run across all components. Else\n",
    "        they are just the number of iterations taken to converge.\n",
    "    Notes\n",
    "    -----\n",
    "    Implementation based on\n",
    "    `A. Hyvarinen and E. Oja, Independent Component Analysis:\n",
    "    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n",
    "    pp. 411-430`\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=None, algorithm='parallel', whiten=True,\n",
    "                 fun='logcosh', fun_args=None, max_iter=200, tol=1e-4,\n",
    "                 w_init=None, random_state=None):\n",
    "        super(FastICA, self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.algorithm = algorithm\n",
    "        self.whiten = whiten\n",
    "        self.fun = fun\n",
    "        self.fun_args = fun_args\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.w_init = w_init\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _fit(self, X, compute_sources=False):\n",
    "        \"\"\"Fit the model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        compute_sources : bool\n",
    "            If False, sources are not computes but only the rotation matrix.\n",
    "            This can save memory when working with big data. Defaults to False.\n",
    "        Returns\n",
    "        -------\n",
    "            X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        fun_args = {} if self.fun_args is None else self.fun_args\n",
    "        whitening, unmixing, sources, X_mean, self.n_iter_ = fastica(\n",
    "            X=X, n_components=self.n_components, algorithm=self.algorithm,\n",
    "            whiten=self.whiten, fun=self.fun, fun_args=fun_args,\n",
    "            max_iter=self.max_iter, tol=self.tol, w_init=self.w_init,\n",
    "            random_state=self.random_state, return_X_mean=True,\n",
    "            compute_sources=compute_sources, return_n_iter=True)\n",
    "\n",
    "        if self.whiten:\n",
    "            self.components_ = np.dot(unmixing, whitening)\n",
    "            self.mean_ = X_mean\n",
    "            self.whitening_ = whitening\n",
    "        else:\n",
    "            self.components_ = unmixing\n",
    "\n",
    "        self.mixing_ = linalg.pinv(self.components_)\n",
    "\n",
    "        if compute_sources:\n",
    "            self.__sources = sources\n",
    "       \n",
    "        return sources\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit the model and recover the sources from X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        return self._fit(X, compute_sources=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self._fit(X, compute_sources=False)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y='deprecated', copy=True):\n",
    "        \"\"\"Recover the sources from X (apply the unmixing matrix).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data to transform, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : (ignored)\n",
    "            .. deprecated:: 0.19\n",
    "               This parameter will be removed in 0.21.\n",
    "        copy : bool (optional)\n",
    "            If False, data passed to fit are overwritten. Defaults to True.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        if not isinstance(y, string_types) or y != 'deprecated':\n",
    "            warnings.warn(\"The parameter y on transform() is \"\n",
    "                          \"deprecated since 0.19 and will be removed in 0.21\",\n",
    "                          DeprecationWarning)\n",
    "\n",
    "        check_is_fitted(self, 'mixing_')\n",
    "\n",
    "        X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n",
    "        if self.whiten:\n",
    "            X -= self.mean_\n",
    "\n",
    "        return np.dot(X, self.components_.T)\n",
    "\n",
    "    def inverse_transform(self, X, copy=True):\n",
    "        \"\"\"Transform the sources back to the mixed data (apply mixing matrix).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_components)\n",
    "            Sources, where n_samples is the number of samples\n",
    "            and n_components is the number of components.\n",
    "        copy : bool (optional)\n",
    "            If False, data passed to fit are overwritten. Defaults to True.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'mixing_')\n",
    "\n",
    "        X = check_array(X, copy=(copy and self.whiten), dtype=FLOAT_DTYPES)\n",
    "        X = np.dot(X, self.mixing_.T)\n",
    "        if self.whiten:\n",
    "            X += self.mean_\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run 'Desvectorize.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=9,random_state=10)\n",
    "S_ = ica.fit_transform(matriz_serpentina) \n",
    "A_ = ica.components_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.14000023e-05,  -6.12465512e-19,   8.26304949e-19,\n",
       "         -2.87331384e-19,  -1.54514731e-06,  -7.55433885e-05,\n",
       "         -5.99602327e-06,  -1.50868937e-04,   1.60142840e-05,\n",
       "          8.18508544e-04,  -4.10301554e-05,  -1.50221113e-05,\n",
       "          1.11259763e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   5.33112068e-29,  -6.30916008e-35,\n",
       "          1.15886611e-05,   2.74917760e-05,   5.22750318e-06,\n",
       "          7.88865039e-06,  -2.58009897e-04,   3.90200628e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -1.80948686e-05,  -2.31986828e-05,  -1.74164822e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -4.07844983e-06,   3.83137617e-05,  -5.32725914e-05,\n",
       "         -2.43772195e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -2.37424783e-04,  -3.83851043e-05,\n",
       "         -7.00323466e-06,   2.00387408e-06,   8.10586895e-05,\n",
       "          3.90200628e-04,  -5.91771221e-04,   2.37850747e-05,\n",
       "          1.18408324e-04,   2.93842986e-05,  -4.60950210e-05,\n",
       "          9.90337468e-06,  -3.17568089e-04,   8.72804338e-07,\n",
       "          1.30399344e-04,  -1.70028676e-05,   5.82208050e-06,\n",
       "         -6.75261294e-05,  -1.42735937e-04,   4.37986057e-05,\n",
       "          3.85906646e-05,  -1.15865097e-04,  -1.02422395e-04,\n",
       "          0.00000000e+00],\n",
       "       [  1.50154449e-05,  -3.73616308e-19,   9.51792058e-19,\n",
       "         -8.72761195e-20,  -1.83818759e-05,   5.42668808e-05,\n",
       "          2.30067474e-05,  -1.33469261e-04,   4.02362587e-05,\n",
       "          4.15805865e-04,  -5.10075472e-05,  -1.56365284e-05,\n",
       "         -6.03645112e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   7.77468014e-29,  -4.19023273e-35,\n",
       "          6.13583021e-05,   4.75524375e-05,   5.54909416e-05,\n",
       "         -8.46440603e-06,  -5.70638135e-04,   1.84466205e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          6.11775338e-06,  -3.40092314e-06,  -1.12721528e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -3.54599721e-05,   3.24395020e-05,   1.13697322e-04,\n",
       "         -7.94224377e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -7.29915232e-05,  -2.24052063e-07,\n",
       "         -2.35748092e-06,   1.08820518e-06,   5.90907641e-05,\n",
       "          1.84466205e-04,  -6.79984136e-04,   5.84107668e-07,\n",
       "          1.24424927e-07,  -3.64505663e-06,  -7.66493244e-06,\n",
       "          4.48410271e-05,  -4.33338537e-05,  -9.30849543e-05,\n",
       "          1.10734399e-05,   2.40957864e-06,  -2.80130172e-05,\n",
       "         -1.20032243e-04,  -3.70350085e-05,   5.79721857e-05,\n",
       "          7.28498068e-05,  -1.67994279e-04,  -8.03839514e-05,\n",
       "          0.00000000e+00],\n",
       "       [  4.77088915e-05,  -6.19826950e-20,   6.82454673e-19,\n",
       "          1.95040214e-19,  -1.43179270e-04,   9.45833120e-05,\n",
       "          5.68618926e-05,  -3.49180055e-05,   5.81395741e-05,\n",
       "          3.87963937e-05,  -3.87717852e-05,  -4.92489651e-05,\n",
       "         -4.66585660e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   7.65013429e-29,   5.51775364e-37,\n",
       "          8.38486340e-05,   6.17736970e-05,   9.60320621e-05,\n",
       "         -1.03803242e-04,  -5.91561935e-05,   2.05685360e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.76009077e-05,   2.94086130e-05,  -7.92081306e-06,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -1.78544917e-04,  -1.78635088e-04,   4.40124225e-04,\n",
       "         -8.15176716e-06,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -5.55744256e-05,   4.63987034e-05,\n",
       "         -2.46309164e-05,  -1.23683426e-05,   8.31395199e-05,\n",
       "          2.05685360e-05,   4.73223748e-05,  -3.27596362e-04,\n",
       "          2.09409477e-05,  -1.56198025e-04,  -1.46221276e-04,\n",
       "          8.67119983e-05,   3.02557143e-05,  -5.20412156e-04,\n",
       "         -3.60813578e-05,  -5.14718938e-06,  -7.76790480e-05,\n",
       "          9.15604558e-05,  -5.18079128e-05,   6.08569548e-05,\n",
       "          7.58830641e-05,   4.44115270e-05,   1.24159830e-05,\n",
       "          0.00000000e+00],\n",
       "       [ -7.01760130e-05,   4.56335365e-19,  -4.72769441e-19,\n",
       "         -9.82143785e-20,   2.81020187e-05,  -1.79265022e-04,\n",
       "         -3.48222366e-06,   3.64737174e-04,  -8.80102758e-05,\n",
       "         -1.99337624e-04,   9.10367135e-05,  -2.01594996e-05,\n",
       "          8.29865946e-06,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -1.07217697e-28,  -4.87125184e-36,\n",
       "         -1.03499941e-04,  -8.73695104e-05,  -1.28004713e-04,\n",
       "         -5.95500575e-05,   1.13303958e-04,   1.18129205e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -6.16157261e-05,   5.62707037e-05,   2.57438861e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          5.28351507e-05,  -2.96143226e-05,  -3.51184104e-04,\n",
       "         -1.05667978e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   1.27145535e-04,   1.81883444e-05,\n",
       "          2.41556093e-04,  -2.10814366e-04,   1.67841351e-05,\n",
       "          1.18129205e-04,  -1.16559029e-06,  -7.50943156e-06,\n",
       "         -2.53037806e-05,   4.88043546e-05,  -1.20885392e-04,\n",
       "         -1.61443287e-04,  -1.83847707e-04,   1.80313267e-04,\n",
       "          2.24066038e-04,  -6.86579000e-05,   1.32131228e-05,\n",
       "          2.65200339e-04,  -8.45520451e-05,  -6.83123751e-05,\n",
       "         -1.28521066e-04,  -6.09127527e-05,   3.06920696e-04,\n",
       "          0.00000000e+00],\n",
       "       [  1.78565272e-05,  -3.43766825e-19,  -6.15306317e-19,\n",
       "         -4.29891791e-19,  -9.30319976e-06,   1.88587025e-05,\n",
       "          7.93583116e-05,  -9.34471200e-05,   7.67725983e-05,\n",
       "          2.50837426e-05,   3.63577910e-05,   2.20163530e-04,\n",
       "         -1.35707152e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   8.15710894e-29,   9.49605223e-36,\n",
       "          4.09768950e-05,   2.74779174e-05,   1.94883800e-04,\n",
       "          6.08265139e-04,  -1.19304031e-04,   2.07324084e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          5.53291012e-05,   1.39080746e-05,  -3.92330863e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -5.77748954e-05,   5.86203459e-06,   4.01213080e-04,\n",
       "          3.38583472e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -5.06967488e-05,   9.54577526e-05,\n",
       "         -5.24449162e-05,  -1.92265486e-05,  -4.94081623e-04,\n",
       "          2.07324084e-05,   5.35835631e-05,   1.02847814e-04,\n",
       "          8.42549153e-05,  -9.40272746e-05,   6.60582692e-04,\n",
       "          1.14817066e-04,  -3.83067099e-06,  -1.69912313e-04,\n",
       "          7.23447134e-06,   3.41312370e-04,   3.25553839e-04,\n",
       "         -1.00203583e-03,  -1.30067950e-05,   9.00677804e-07,\n",
       "          9.46401216e-05,   8.62075697e-05,  -6.07292068e-05,\n",
       "          0.00000000e+00],\n",
       "       [ -9.56915137e-06,   1.06809156e-20,  -1.78749258e-19,\n",
       "         -3.13550136e-20,  -1.43484915e-05,  -3.10104458e-05,\n",
       "         -2.21635138e-06,   7.32001712e-06,  -1.39816564e-05,\n",
       "         -6.23162666e-06,   2.27205707e-05,   1.94924558e-05,\n",
       "          4.38718523e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -1.83733529e-29,   5.34090514e-36,\n",
       "         -2.72340928e-05,  -1.97398121e-05,  -6.39404468e-07,\n",
       "          1.77327619e-05,   1.12388161e-04,   2.10558295e-07,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.85680526e-06,  -5.85591728e-06,   9.10660503e-06,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -2.81490012e-05,   1.81350481e-05,   3.26830167e-04,\n",
       "          2.09995096e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   3.77328395e-05,   5.13806658e-06,\n",
       "          1.07554330e-05,   3.69304316e-05,  -8.02701381e-05,\n",
       "          2.10558295e-07,   9.69240098e-05,  -1.19912177e-05,\n",
       "          1.75212573e-05,  -4.33753868e-05,   1.68254917e-04,\n",
       "         -1.09863206e-05,   5.28351591e-06,  -7.77844876e-05,\n",
       "          2.03366198e-05,   3.99607845e-05,   1.09633446e-05,\n",
       "          4.23935407e-05,   3.22851707e-05,  -2.66435898e-05,\n",
       "         -2.89307078e-05,   2.82839740e-05,  -8.31318405e-06,\n",
       "          0.00000000e+00],\n",
       "       [  1.24779208e-05,  -1.12756967e-19,   4.31472206e-19,\n",
       "          2.20174783e-19,   6.57991035e-06,   3.36556854e-05,\n",
       "         -2.14797761e-05,   1.17430607e-05,  -1.10797288e-05,\n",
       "          1.44496637e-05,  -3.56760355e-05,  -1.72803859e-04,\n",
       "         -1.43193327e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -6.36859977e-30,  -5.15508800e-36,\n",
       "          1.64239072e-05,   1.47952403e-05,  -5.25266551e-05,\n",
       "         -5.68235495e-04,   2.13656694e-05,  -1.16369736e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -1.98549940e-05,  -2.44419703e-06,  -1.00619470e-06,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.25634793e-05,   2.76971976e-05,   5.74915071e-05,\n",
       "         -2.07875264e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -1.08192685e-05,  -2.98726908e-05,\n",
       "         -8.92896847e-07,   6.16840603e-06,   1.29047239e-04,\n",
       "         -1.16369736e-05,  -2.05110037e-05,  -7.02452817e-06,\n",
       "         -3.43971019e-05,   2.19102259e-05,  -2.46289329e-04,\n",
       "         -1.79872063e-05,   1.42723271e-05,   3.68229451e-05,\n",
       "         -2.98904759e-05,  -2.20749463e-04,  -4.24779744e-04,\n",
       "          3.94964960e-04,  -1.52967891e-05,   2.79928154e-05,\n",
       "         -4.07936485e-06,  -2.73850936e-05,   1.47028147e-05,\n",
       "          0.00000000e+00],\n",
       "       [ -2.49260306e-05,  -3.43957908e-19,  -1.49531954e-20,\n",
       "         -3.41277322e-19,  -1.39519881e-05,  -2.00979917e-04,\n",
       "          2.71941464e-05,   1.05189682e-04,  -1.22280908e-05,\n",
       "          2.76627258e-04,   3.19706264e-05,  -8.89228098e-07,\n",
       "          5.76909495e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -1.01221184e-30,  -3.42574335e-35,\n",
       "         -4.54559993e-05,  -2.36270406e-05,  -8.06781992e-06,\n",
       "         -3.72921089e-07,   4.45798372e-05,   2.47317359e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -5.81015949e-05,   2.92886185e-05,   1.30591773e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -1.96379120e-05,   3.59829898e-06,   8.16736229e-05,\n",
       "         -3.08034194e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -2.90742055e-04,   1.42426477e-05,\n",
       "          3.23253346e-05,  -3.11469489e-04,  -3.21784260e-05,\n",
       "          2.47317359e-04,  -1.23265763e-04,  -1.48788756e-05,\n",
       "          9.87955854e-05,  -8.71026712e-06,   4.74706451e-05,\n",
       "         -6.70184811e-05,  -4.98494991e-04,  -4.37504181e-05,\n",
       "          1.99447860e-04,   3.87964147e-06,   2.94481293e-07,\n",
       "         -2.28341525e-05,  -2.93905025e-04,  -3.94205582e-07,\n",
       "         -1.56088495e-05,  -1.66167360e-05,   1.10501185e-04,\n",
       "          0.00000000e+00],\n",
       "       [  4.23476079e-04,  -1.76202005e-18,   2.28774024e-18,\n",
       "          4.17028715e-19,  -4.23491436e-05,   8.72362742e-04,\n",
       "          4.70669404e-04,  -1.95337187e-04,   5.47872679e-04,\n",
       "          2.94170808e-04,  -2.80598268e-04,  -7.68225088e-05,\n",
       "         -4.53652578e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   6.28358628e-28,   8.45793506e-36,\n",
       "          7.64043230e-04,   5.64317973e-04,   7.80641175e-04,\n",
       "         -6.88315199e-05,  -5.83047123e-04,   1.86791032e-04,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.71621251e-04,   2.79563391e-04,  -1.57513141e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         -4.65995884e-05,   3.56217653e-04,   2.01773990e-04,\n",
       "         -6.31306412e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,  -5.07661793e-04,   3.61432616e-04,\n",
       "         -1.87581622e-04,  -1.34902460e-04,   9.41376484e-04,\n",
       "          1.86791032e-04,   3.84316606e-04,   2.02362446e-04,\n",
       "          1.98690284e-04,   1.45023739e-04,  -6.07000105e-04,\n",
       "          7.69532680e-04,   3.02145644e-04,  -4.50108638e-04,\n",
       "         -2.92975911e-04,  -5.35322044e-07,  -2.69401423e-04,\n",
       "         -9.06449135e-04,  -4.70538119e-04,   5.68441469e-04,\n",
       "          8.65366604e-04,   3.97060446e-04,   1.99472638e-04,\n",
       "          0.00000000e+00]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
